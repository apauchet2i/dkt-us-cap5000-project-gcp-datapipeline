steps:
# 1. Fetch the source code
- name: gcr.io/cloud-builders/git
  args: ['clone', 'https://github.com/apauchet2i/gcp-batch-ingestion-bigquery-master.git']

# 2a. Set up GCS & BQ etc. using public terraform Docker image
- name: hashicorp/terraform
  args: ['init']
  dir: 'terraform'

# 2b. Create the GCS bucket using Terraform
- name: hashicorp/terraform
  id: terraform-apply
  args: ['apply', '-auto-approve']
  dir: 'terraform'

  # 3. Build and run the Dataflow pipeline (staged template)
- name: gcr.io/cloud-builders/gradle
  id: build-dataflow-pipeline
  args: ['build', 'runTemplatePipelineOrders']
  waitFor: ['terraform-apply']

# 4b. Deploy the Cloud Function that listens to the bucket
- name: gcr.io/cloud-builders/gcloud
  id: function-deploy-createjob
  args: ['functions', 'deploy', 'createJob', '--trigger-event=google.storage.object.finalize', '--stage-bucket=gs://dkt-us-ldp-baptiste-test', '--trigger-resource=gs://dkt-us-ldp-baptiste-test', '--runtime=nodejs12','--service-account=data-collector@dkt-us-data-lake-a1xq.iam.gserviceaccount.com']
  dir: 'cloud-function/createJob'
  waitFor: ['build-dataflow-pipeline']

  # 4b. Deploy the Cloud Function that listens to the bucket
- name: gcr.io/cloud-builders/gcloud
  id: function-deploy-deduplicatedata
  args: ['functions', 'deploy', 'deduplicateData', '--trigger-topic=dkt-us-cap5000-project-end-datapipeline', '--runtime=nodejs12']
  dir: 'cloud-function/deduplicateData'
  waitFor: ['build-dataflow-pipeline']